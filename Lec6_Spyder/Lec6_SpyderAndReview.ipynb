{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lecture 10 \n",
    "ENVR 890-010: Python for Environmental Research, Fall 2021\n",
    "\n",
    "November 5, 2021\n",
    "\n",
    "By Rosa Cuppari. Some material adapted from Andrew Hamilton, Greg Characklis, David Gorelick and H.B. Zeff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this lecture we will be transitioning from Jupyter Notebooks to Spyder, an environment for coding in Python. Spyder should have been downloaded to your computers along with Anaconda. We will also be reviewing and reinforcing concepts from the rest of the class and introducing GitHub. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief statistics interlude\n",
    "During our regression lecture, we mentioned many important tests to run on data to evaluate normality as well as indicator values for understanding whether our regression is statistically significant. To review what the important values mean, here is a list of key terms:\n",
    "\n",
    "1. **p-value:** the \"significance\" level. This is simply the probability of seeing the result you are getting - particularly the t-value - in a random dataset (e.g., if the coefficients were all zero). If it is very low (e.g., <0.05), the chances of  your analysis being statistically significant are high. \n",
    "1. **t-test:** based on the Student's t-statistic and t-distribution. It calculates a p-value to accept/reject the null hypothesis. It essentially compares the mean with the mean that a sample of your size is expected to have. \n",
    "1. **f-test:** based on a different distribution that the t-test, it is used to compare variances, instead of means, comparing your regression coefficients to an equation with coefficients set to zero. Note: unlike the p-value that looks at a single variable at a time, the f-statistic is evaluating the *overall* significance of your results (i.e. all the variables together). \n",
    "1. **r2 versus adjusted r2:** r2 is the correlation coefficient squared. The adjusted r2 takes into account how many variables you are inputting to your regression equation to account for potential overfitting. \n",
    "1. **Log-likelihood:** the higher the better. You can use this to compare two relatively simple models, as it is a way to describe the joint probability of your data and the coefficients in your model.  \n",
    "1. **Aikake Information Criterion (AIC):** a way to measure how good your model compared to others, describing \"information loss.\" The lower the better! It includes a \"penalty\" for more parameters.\n",
    "1. **Bayesian Information Criterion (BIC):** similar to AIC, it describes the quality of the model, also penalizing the model for more parameters (a bit more than AIC). The lower the better! \n",
    "1. **Skew:** how asymmetrical your data is. Between -3 and 3 is ok, otherwise your data may not be normally distributed.\n",
    "1. **Kurtosis:** how \"tall\"/\"peaked\" your distribution is. Between -3 and 3 is ok, otherwise your data may not be normally distributed.\n",
    "1. **Durbin-Watson:** a test for autocorrelation in model residuals at a lag of 1. The values range from 0 to 4, with <2 indicating a positive autocorrelation, >2 indicating a negative autocorrelation, and 2 indicating no autocorrelation.\n",
    "1. **Jarque-Bera:** a test for normal skewness and kurtosis. The further from zero it is, the more non-normal. \n",
    "1. **Omnibus:** another test for normality! A value close to zero is good for the Omnibus test and close to one is good for the Prob (Omnibus).\n",
    "\n",
    "Some of the infinitely many resources available: \n",
    "1. Brief Princeton U Library Guides: [Regression Intro](https://dss.princeton.edu/online_help/analysis/regression_intro.htm), [Interpreting Regressions](https://dss.princeton.edu/online_help/analysis/interpreting_regression.htm) (*they need a website update*)\n",
    "1. [A very long linear regression 101 guide](https://dss.princeton.edu/training/Regression101.pdf) (*sadly based in STATA*)\n",
    "1. [Statistics How To](https://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/) (*as an intro, easy read*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now, Spyder! \n",
    "Spyder is useful because: \n",
    "1. It's easier to keep track of the variables you have created and you can easily see how they are categorized\n",
    "1. I find it's a little bit faster \n",
    "1. You can look into your folders more easily\n",
    "1. You don't use cells (I find they make it easier to lose track of things) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note: Spyder will automatically set your directory as the folder the file is saved in!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going backwards: applications in Spyder \n",
    "We have learned a ton of stuff this semester, but we have focused on Jupyter Notebooks. Implementation in Spyder, as you have seen, is nearly identical, so we are going to take advantage of these similarities to review some practical skills in Spyder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise: reading in multiple files in Spyder\n",
    "First, let's read in our data. I have collected data from the [World Bank Open Data Portal](https://data.worldbank.org/). The data is really nice because it's global, it is downloaded with consistent columns (i.e. 1960 onwards), and you can find some long timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's crowdsource what we should import \n",
    "## hint: there are two/three you should essentially always import\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list with the names of each data file in the folder \n",
    "names = ['AgLand','CO2','Electricity','Freshwater','GDP','Population','Poverty','Renewables',\n",
    "        'Traffic_Mortality','UrbanPop','WaterSan','MaternalMortality','Literacy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an empty dataframe to hold all of our information \n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-fea5a6396c46>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-fea5a6396c46>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    df = df.iloc[:,:65] ##need to drop last column to clean data\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Create a for loop which takes the names of the indicators and reads in the csv files for each one, \n",
    "## concatenating them into one large dataframe\n",
    "\n",
    "## Set up what the for loop is going to iterate over \n",
    "for n in names: \n",
    "    ## Within the for loop, write a line to read in the file where the file name is changing according to \n",
    "    ## the indicator. Skip 3 lines.\n",
    "    ## hint: you will need to combine a non-string with the indicator name with a string ('.csv')\n",
    "    new = pd.read_csv(##INSERT NAME!)\n",
    "    ## Within the for loop, concatenate your files based on rows. \n",
    "    ## hint: pd.concat([df1,df2]) will concatenate based on rows \n",
    "    ## the paramater axis=0 is implied, changing this to  axis=1 will concatenate based on columns.\n",
    "    df = \n",
    "\n",
    "df = df.iloc[:,:65] ##need to drop last column to clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print data head and data tail once you are done, checking to see that the columns have consistent values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For those that want to practice: do this without writing the names explicitly into a list. Hint: you can iterate over all of the files in your directory.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise: subsetting and plotting data \n",
    "Now we are going to subset our data for Afghanistan, Brazil, Niger, and Bangladesh, and then plot them. We will be using a function that takes the names of countries as a list and the variable of interest, and then uses a for loop to create a line plot with each of the countries displayed. This, of course, needs a legend so that we can interpret it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Afghanistan', 'Brazil']\n"
     ]
    }
   ],
   "source": [
    "## define your function with three inputs: the dataset to use, countries of interest, and indicator of interest\n",
    "## hint: make sure you know what the indicators are actually called in your dataset. \n",
    "## df.column.unique() could be helpful\n",
    "countries = ['Afghanistan','Lebanon','Colombia','Brazil','Rwanda','Niger','Nepal','Bangladesh']\n",
    "\n",
    "def country_plots(df, countries, variable): \n",
    "    ## Start by isolate our indicator variable \n",
    "\n",
    "    \n",
    "    ## write a line to subset the data based on whether the country column contains any of the \n",
    "    ## names within our list. There are a few ways to do this, but in order for the function to be useful, \n",
    "    ## you will probably want to find the number of values in the countries list and iterate over them\n",
    "    \n",
    "    df_subset = pd.DataFrame()\n",
    "    for n in countries: \n",
    "        df_new = \n",
    "        \n",
    "        ## For our purposes later, you will want to TRANSPOSE the data, that is flip it so that the \n",
    "        ## years are all in the first column. I have added the line below, meant to happen AFTER you subset the df\n",
    "        ## I have also named your column \n",
    "        df_transposed = df_new.T\n",
    "        df_transposed.columns = [str(n)] \n",
    "        ## you will also need to drop the first four rows (in your spare time, check out what happens if you do not)\n",
    "        df_transposed = df_tranposed.iloc[5:,0]\n",
    "        \n",
    "        df_subset = ## this will be your combine dataset with year as index and columns as country \n",
    "        ## hint: concatenate the old with the new \n",
    "\n",
    "    ## create a line plot with different lines for each country over time\n",
    "    ## hint: you can do this a few different ways, but a for loop is a safe bet (feel free to play around with this)\n",
    "    \n",
    "    ## also create a boxplot where the x axis is the year and the y axis is the spread for all of the countries combined \n",
    "    \n",
    "    return df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A thought: what would you do if you wanted to create a new variable for each country?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another bonus exercise: can you create a nested dictionary where the key is the country, then you sort by indicator, and then the final layer is the value? How would you plot this using a for loop?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A final bonus! How would you make this function spit out plots for many variables? (or even all the indicators in our folder?)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise: regression analysis \n",
    "What if we want to read in all of the indicators for the same country and test the relationships between them? Let's give it a try: reading in all of the files and selecting for a single country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start by reading in all of the data files using a similar for loop to the first exercise \n",
    "## (i.e. iterating over the indicators)\n",
    "\n",
    "\n",
    "## subset by COUNTRY this time instead of by indicator \n",
    "\n",
    "## manipulate the data so that the years are the index/rows, and the columns are the indicator names \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let us regress! Import the necessary packages \n",
    "\n",
    "## pick 3-5 variables and conduct a regression analysis to pick one (there is a variety of data here) \n",
    "## make sure to consider whether time (years) are the predominant influence on outcome (and why!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus\n",
    "Can you create a for loop that will automatically create a regression for each variable? What could you do if you wanted to run many regressions and record the results without printing them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub Intro\n",
    "Great! Now we have some cool code we want to share with the world because other people might want to replicate our analysis (yay for FAIR data and transparent science!). You might wonder whether you can publish Jupyter Notebooks somewhere, or Spyder analyses. The answer is yes: on GitHub. In fact, you can find the lectures we use in [Andrew's online repository](https://github.com/ahamilton144/Python-For-Environmental-Research), though changes from last year to this year are still not publically available. In fact, we will be using another one of Andrew's repositories - [his Git Tutorial](https://github.com/ahamilton144/GitTutorial) - to guide us for the second half of this lecture and the first half of next week's lecture.   \n",
    "\n",
    "GitHub is also really useful as an online database for your own work. You can update code as you work on it so that if your computer crashes or if your external hard drive breaks, you have a back up option. You can keep repositories private, so that others cannot see your work in progress. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
